{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqNYPFSANcDb"
   },
   "source": [
    "\n",
    "## What is Web Scraping?\n",
    "* Extracting data from internet\n",
    "* Extracted data is collected and then changed into a suitable format that is useful for the user (i.e. CSV)\n",
    "* Extract all data from the page or specific data selected by the user before it is run\n",
    "* Specific data requires techniques to identify CSS and Javascript element corresponds to required data\n",
    "* User checks through the data, confirming scraper works properly\n",
    "* Web scraper outputs the data collected\n",
    "* Collected data can then be changed into a suitable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2q5eIaojOM_9"
   },
   "source": [
    "What are web scrapers used for\n",
    "* Extracting information from the net\n",
    "* Depending problem statement and the type of analysis the data will be run on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZmKWOr0OiLY"
   },
   "source": [
    "Types of websites\n",
    "* Static: the content of page does not change e.g. history sites\n",
    "* Dynamic: content of the page, hence it is never the same at any point of time e.g. e-commerce sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWVPrYuWO43Y"
   },
   "source": [
    "Beautiful Soup\n",
    "* One of the most commonly used parsing libraries\n",
    "* Very useful in pulling out information from the HTML page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1DzjSQvPFhV"
   },
   "source": [
    "## 1. Download/Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement urllib (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for urllib\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.6\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Downloading charset_normalizer-3.4.0-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.4.0 idna-3.10 requests-2.32.3 urllib3-2.2.3\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pprint (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pprint\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install urllib\n",
    "!pip install bs4\n",
    "!pip install requests\n",
    "!pip install pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaARqfq3OyqJ"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZRBLU8uP5Nl"
   },
   "source": [
    "## 2. Scraping Basic Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyYqpV35P4eH"
   },
   "outputs": [],
   "source": [
    "# 1. Scraping HTML\n",
    "url = \"https://forecast.weather.gov/MapClick.php?lat=37.7772&lon=-122.4168\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuZ7xgfGU7zm"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Informational responses (100–199)\n",
    "#Successful responses (200–299)\n",
    "#Redirection messages (300–399)\n",
    "#Client error responses (400–499)\n",
    "#Server error responses (500–599)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ph4sL3ptQRlv"
   },
   "outputs": [],
   "source": [
    "#2. Make HTML looks more presentable / has indentation\n",
    "\n",
    "# print(soup) # Just to compare between using and not using prettify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1anM08d-QtEv"
   },
   "outputs": [],
   "source": [
    "# 3. How to find information of tags from HTML\n",
    "#  a) Finding all instances of a tag using find_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "027zz-RVRGcN"
   },
   "outputs": [],
   "source": [
    "# get_text() function extract text\n",
    "\n",
    "\n",
    "# Count the statements in between the \"p\"s, notice the index 1 statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3hioZBGXcsG"
   },
   "outputs": [],
   "source": [
    "# Get statement from index 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsmdCzeLRch0"
   },
   "outputs": [],
   "source": [
    "# 3b) Finding the first instance of tag using find()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIrq5ZRBYGz1"
   },
   "outputs": [],
   "source": [
    "# This is a get_text function, all the those with =\"\" are not text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDDKLRJDeNRF"
   },
   "outputs": [],
   "source": [
    "# Split up a string into a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WnMl-w2WR14s"
   },
   "outputs": [],
   "source": [
    "#c) Search for tags by class or id\n",
    "\n",
    "# Find tags with class period-name\n",
    "\n",
    "# Reason why the class does not require quotations: it is a CSS selector, hence recognised by the Python language\n",
    "# The reason why we put an underscore after the class is because class is a function inbuilt in the Python system, \n",
    "# therefore underscore '_' tells the system to get the CSS selector instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msX6kWXqSKWv"
   },
   "outputs": [],
   "source": [
    "# Find tags with id news-items\n",
    "\n",
    "# Get whatever it is under or nested in the id 'news-items' (Refer to the soup code)\n",
    "# Reason why id is not required a quotation: it is a CSS selector, hence recognised by the Python language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1lrIRzhSV1e"
   },
   "source": [
    "## 3. Scraping for real now\n",
    "1. Download webpage containing the forecast\n",
    "2. Create a BeautifulSoup Class to parse the page\n",
    "3. Find the div with id seven-day-forecast and assgin to seven-day\n",
    "4. Inside seven-day, find each individual forecast item\n",
    "5. Extract and print the first forecast item\n",
    "6. Using the tag information found from Step 5, extract the following information: Period, Short Description, Temperature and Description of the condtions\n",
    "7. Format the extracted data into a pandas dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjH5idb-SVic"
   },
   "outputs": [],
   "source": [
    "# Show our current soup variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTWtdGhcgBta"
   },
   "outputs": [],
   "source": [
    "# Find the div with id seven-day-forecast and assign to seven-day\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHzxQrVlgeTU"
   },
   "outputs": [],
   "source": [
    "# Step 4: Inside the seven day, find each individual forecast item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_7ONqzOi638"
   },
   "outputs": [],
   "source": [
    "# Step 5: Extract and print the first forecast item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUV8cCqxjfZX"
   },
   "outputs": [],
   "source": [
    "# Step 6: Using the tag information found from Step 5, extract the following information:\n",
    "# Period, Short Description, Temperature and Description of the condtions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpBvn_09kJqo"
   },
   "outputs": [],
   "source": [
    "# Description of the conditions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5qSwnJekBmB"
   },
   "outputs": [],
   "source": [
    "# Extract all period names\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyh8hjHbnAjw"
   },
   "outputs": [],
   "source": [
    "# Create our variables short_descs, temps, descs\n",
    "\n",
    "# We need put the . for class and id, because they are CSS selectors\n",
    "\n",
    "# Do not have to put the dots there because img is not under the division of the tombstone-container\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "APAm3FchoA4j"
   },
   "outputs": [],
   "source": [
    "# Step 7: Format the extracted data into a pandas dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "fqNYPFSANcDb",
    "g1DzjSQvPFhV",
    "WZRBLU8uP5Nl",
    "I1lrIRzhSV1e"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
